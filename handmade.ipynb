{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IREN - Handmade model\n\nGroup members : Adrien MERAT (SCIA) & Erwan GOUDARD (SCIA)\n\nThis project aims to classify ships pictures using neural networks.\n\n## 1. Libraries imports","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport random as rd\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Normalization\nimport skimage\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-10T08:04:12.448775Z","iopub.execute_input":"2022-07-10T08:04:12.449556Z","iopub.status.idle":"2022-07-10T08:04:19.100411Z","shell.execute_reply.started":"2022-07-10T08:04:12.449451Z","shell.execute_reply":"2022-07-10T08:04:19.09951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Data loading and analysis\n\nWe begin by importing the data from the kaggle directory and extract the tarball.","metadata":{}},{"cell_type":"code","source":"!tar xzf /kaggle/input/navires-2022-a-la-mano/ships.tgz\n!ls /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:04:19.1027Z","iopub.execute_input":"2022-07-10T08:04:19.103013Z","iopub.status.idle":"2022-07-10T08:04:24.153146Z","shell.execute_reply.started":"2022-07-10T08:04:19.102969Z","shell.execute_reply":"2022-07-10T08:04:24.152014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After manually looking at some of the pictures, I have noticed that most of them are colored while some of them are in grey scale. This can make our learning process harder and a simple solution is to get rid of those grey scale images.\n\nWe can define a function to test if an image is RGB or grey.","metadata":{}},{"cell_type":"code","source":"def is_rgb(img):\n    \"\"\"\n    Check if a given image is RGB/colored\n    \"\"\"\n    return len(img.shape) == 3 and img.shape[2] == 3","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:04:24.155622Z","iopub.execute_input":"2022-07-10T08:04:24.156087Z","iopub.status.idle":"2022-07-10T08:04:24.163533Z","shell.execute_reply.started":"2022-07-10T08:04:24.156Z","shell.execute_reply":"2022-07-10T08:04:24.161416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then count how many grey scale image we have and compare it to the number of all images.","metadata":{}},{"cell_type":"code","source":"def describe(path):\n    \"\"\"\n    Count the number of images as well as the number of grey scale images\n    \"\"\"\n    images = []\n    non_rgb = []\n    \n    for directory in np.sort(os.listdir(path)):\n        directory = os.path.join(path, directory)\n        \n        for file in os.listdir(directory):\n            file = os.path.join(directory, file)\n            image = skimage.io.imread(file)\n            images.append(image)\n            \n            if not is_rgb(image):\n                non_rgb.append(image)\n                \n    width = [img.shape[0] for img in images]\n    height = [img.shape[1] for img in images]\n                \n    print(f\"Number of images : {len(images)}\")\n    print(f\"Number of grey scale images : {len(non_rgb)}\")\n    print(f\"Ratio of grey scale images over all images : {round(100*len(non_rgb)/len(images), 2)}%\")\n    print(f\"Minimum image width : {min(width)} px\")\n    print(f\"Maximum image width : {max(width)} px\")\n    print(f\"Minimum image height : {min(height)} px\")\n    print(f\"maximum input height : {max(height)} px\")\n\ntrain_path = \"/kaggle/working/ships32\"\ndescribe(train_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:04:24.167587Z","iopub.execute_input":"2022-07-10T08:04:24.168021Z","iopub.status.idle":"2022-07-10T08:04:55.668205Z","shell.execute_reply.started":"2022-07-10T08:04:24.167976Z","shell.execute_reply":"2022-07-10T08:04:55.666986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Thus we can see that all training images have the same size and that the number of grey scale images is quite insignifiant (only 1.86%) which let us remove them without loosing a lot of data to train with. Moreover, using such unappropriate data could make our model return false results.\n\nWe can define a function to prepare the data by creating a dataframe with some of its metadata, a list of the images, a list of the labels (as numbers) and a map to correspond label numbers with their corresponding name. This function automatically remove ","metadata":{}},{"cell_type":"code","source":"def prepare_data(path):\n    \"\"\"\n    Extract information of the images and remove grey scale images\n    \n    :path: Path to the directory containing subdirectories where images are saved\n    :return:\n        :images: List of the RGB images\n        :labels: List of labels of the images\n        :df: A dataframe holding caracteristics of the images\n        :label_map: A map to link labels to they category name\n    \"\"\"\n    labels = []\n    images = []\n    metadata = []\n    current_label = 0\n    label_map = {}\n    \n    for directory in np.sort(os.listdir(path)):\n        label_map[current_label] = directory\n        directory = os.path.join(path, directory)\n        \n        for file in os.listdir(directory):\n            file = os.path.join(directory, file)\n            image = skimage.io.imread(file)\n            \n            if not is_rgb(image):\n                os.remove(file)\n            else:\n                labels.append(current_label)\n                images.append(image)\n                metadata.append([file, current_label, image.shape[0], image.shape[1], image.dtype])\n                \n        current_label += 1\n            \n    columns = [\"filename\", \"category\", \"width\", \"height\", \"type\"]\n    df = pd.DataFrame(metadata, columns=columns)\n    \n    return df, images, np.array(labels), label_map\n\ndf, images, labels, label_map = prepare_data(train_path)\n\nprint(f\"Number of loaded images : {len(images)}\")\nprint(f\"Number of labels : {len(label_map)}\")\nprint(\"List of labels and their corresponding number :\")\nlabel_map","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:04:55.670602Z","iopub.execute_input":"2022-07-10T08:04:55.670972Z","iopub.status.idle":"2022-07-10T08:05:25.814547Z","shell.execute_reply.started":"2022-07-10T08:04:55.670893Z","shell.execute_reply":"2022-07-10T08:05:25.8133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our images have all the same size which will make our work way more easier. Now let's take a look at the competition images :","metadata":{}},{"cell_type":"code","source":"competition_path = '/kaggle/working/ships_competition.npz'\ncompetition = np.load(competition_path, allow_pickle=True)[\"X\"]\ncompetition = competition.astype(float) / 255\nprint(f\"Number of images : {competition.shape[0]}\")\nprint(f\"Images width : {competition.shape[1]} px\")\nprint(f\"Images height : {competition.shape[2]} px\")\nprint(f\"Number of color channels : {competition.shape[3]}\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:25.816611Z","iopub.execute_input":"2022-07-10T08:05:25.817019Z","iopub.status.idle":"2022-07-10T08:05:25.866811Z","shell.execute_reply.started":"2022-07-10T08:05:25.816942Z","shell.execute_reply":"2022-07-10T08:05:25.865688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Competition images are in the same format as training ones, which is perfect.\n\nWe will now see how well are the different categories represented among our dataset.","metadata":{}},{"cell_type":"code","source":"plot = df['category'].value_counts(sort=False).plot.bar()\nplot.set_xlabel(\"Categories of boat\")\nplot.set_ylabel(\"Number of images\")\nplot.set_title(\"Number of images per category of boat\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:25.868655Z","iopub.execute_input":"2022-07-10T08:05:25.868974Z","iopub.status.idle":"2022-07-10T08:05:26.203844Z","shell.execute_reply.started":"2022-07-10T08:05:25.868931Z","shell.execute_reply":"2022-07-10T08:05:26.202899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The 8th category (sailing boat) seems to be the least represented one, we print here its count :","metadata":{}},{"cell_type":"code","source":"min_img = len(df[df['category'] == 8])\nmin_img","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:26.205761Z","iopub.execute_input":"2022-07-10T08:05:26.206088Z","iopub.status.idle":"2022-07-10T08:05:26.216061Z","shell.execute_reply.started":"2022-07-10T08:05:26.206046Z","shell.execute_reply":"2022-07-10T08:05:26.214883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our dataset is not well balanced at all which can reduce our model quality. We can fix this by augmenting our dataset on the least represented categories.\n\n## 3. Data transformations\n\nBefore proceeding to the data augmentation we need to split the dataset in three parts : train set, validation set and test set. Augmenting the dataset before splitting it will cause data leakage as the synthetic data consists of linear combinations of your training data. Our classifier will perform outstanding on your incorrectly created test data and later poorly on real unseen data in production. That's why we first split our dataset.\n\n### 3.1. Splitting dataset\n\nWe will use `keras.ImageDataGenerator` to handle our 3 datasets. It requires to have 2 directories, one for train and validation and the other one for ths test. Thus we will create two folders `/kaggle/working/train` and `kaggle/working/test`. The test folder needs to have the same number of images per category to be unbiased.","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/test\n!cp /kaggle/working/ships_32 /kaggle/working/train","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:26.217985Z","iopub.execute_input":"2022-07-10T08:05:26.218656Z","iopub.status.idle":"2022-07-10T08:05:27.736324Z","shell.execute_reply.started":"2022-07-10T08:05:26.218614Z","shell.execute_reply":"2022-07-10T08:05:27.735203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_dataset(input, output, label_map, img_per_category=None):\n    \"\"\"\n    Split the dataset by moving some images from the input (train)\n    directory to the output (test) directory.\n    \n    :input: The path to the input folder\n    :output: The path to the output folder\n    :label_map: A map between label numbers and name on our dataset\n    :img_per_category: Number of images per category we want in our datasets\n    \"\"\"\n    for key, category in label_map.items():\n        category_path = os.path.join(input, category)\n        test_category_path = os.path.join(output, category)\n        os.mkdir(test_category_path)\n        \n        for file in os.listdir(category_path)[:img_per_category]:\n            os.rename(os.path.join(category_path, file), os.path.join(test_category_path, file))\n            \ntest_path = \"/kaggle/working/test\"\nsplit_dataset(train_path, test_path, label_map, 100)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:27.741595Z","iopub.execute_input":"2022-07-10T08:05:27.742296Z","iopub.status.idle":"2022-07-10T08:05:27.813802Z","shell.execute_reply.started":"2022-07-10T08:05:27.742242Z","shell.execute_reply":"2022-07-10T08:05:27.812866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We choose to take 200 images of each category and move them in the test folder.\n\n### 3.2. Data augmentation\n\nIn order to augment create a better model, we will augment the size of ourr dataset by applying random operators to some images such as horizontal flip, noise, rotation (small) or constrast. Because our images are very small, it would make no sense to use random zoom and because boats floats on water, the use of huge angle rotatiosn, as weel as vertical flip would make no sense in our use case.\n\nWe will also need preprocess our images in order to scale them to fit them into the interval $[0,1]$ as scaled dataset always produce better results. We will use the Keras `ImageDataGenerator` to handle images loading and preprocessing as it load data on the fly which is more memory efficient.\n\n#### 3.2.1. Test dataset\n\nFor the test dataset, we don't need to apply data augmentation as they will be used to check the model's predictions.","metadata":{}},{"cell_type":"code","source":"!mkdir /kaggle/working/keras_test","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:27.815327Z","iopub.execute_input":"2022-07-10T08:05:27.815631Z","iopub.status.idle":"2022-07-10T08:05:28.578627Z","shell.execute_reply.started":"2022-07-10T08:05:27.81559Z","shell.execute_reply":"2022-07-10T08:05:28.577317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator\n\ntest_imgdatagenerator = ImageDataGenerator(rescale=1./255, dtype='float32')\n\ntest_generator = test_imgdatagenerator.flow_from_directory(\n        test_path,\n        target_size=(32, 32),\n        seed=SEED,\n        shuffle=False,\n        save_to_dir='/kaggle/working/keras_test',\n        save_format=\"jpeg\",\n        interpolation=\"bicubic\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:28.581019Z","iopub.execute_input":"2022-07-10T08:05:28.581447Z","iopub.status.idle":"2022-07-10T08:05:28.694457Z","shell.execute_reply.started":"2022-07-10T08:05:28.5814Z","shell.execute_reply":"2022-07-10T08:05:28.693431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.2. Train Dataset\n\nThis time, we will need to split the train dataset in a train and validation set. We choose to teake respectively 90% and 10% of the current train dataset. On top of that we will do some data augmentation to improve the model by duplicating some images and applying them some minor transformations.","metadata":{}},{"cell_type":"code","source":"def add_noise(img):\n    '''Add random noise to an image'''\n    VARIABILITY = 10\n    deviation = VARIABILITY * rd.random()\n    noise = np.random.normal(0, deviation, img.shape)\n    img += noise\n    np.clip(img, 0., 255.)\n    return img\n\ntrain_imgdatagenerator = ImageDataGenerator(rescale=1./255,\n                                            width_shift_range=.2,\n                                            height_shift_range=.2,\n                                            horizontal_flip=True,\n                                            rotation_range=0,\n                                            validation_split=0.1,\n                                            featurewise_center=True,\n                                            preprocessing_function=add_noise,\n                                            dtype='float32')\n\ntrain_generator = train_imgdatagenerator.flow_from_directory(\n        train_path,\n        batch_size=32,\n        target_size=(32,32),\n        seed=SEED,\n        subset=\"training\",\n        interpolation=\"bicubic\")\n\nvalidation_generator = train_imgdatagenerator.flow_from_directory(\n        train_path,\n        batch_size=32,\n        target_size=(32,32),\n        seed=SEED,\n        subset=\"validation\",\n        interpolation=\"bicubic\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:28.696253Z","iopub.execute_input":"2022-07-10T08:05:28.696613Z","iopub.status.idle":"2022-07-10T08:05:30.34877Z","shell.execute_reply.started":"2022-07-10T08:05:28.696571Z","shell.execute_reply":"2022-07-10T08:05:30.347674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3.2.3. Preprocessing\n\nHere we setup preprocessing for our model inference.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import RandomFlip, RandomRotation, RandomContrast, Rescaling\n\npreprocessing = Sequential([\n    Rescaling(1.0 / 255),\n    RandomRotation(25./360, seed=SEED),\n    RandomContrast(0.75, seed=SEED),\n    RandomFlip(\"horizontal\", seed=SEED)])","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:30.350488Z","iopub.execute_input":"2022-07-10T08:05:30.351021Z","iopub.status.idle":"2022-07-10T08:05:33.750361Z","shell.execute_reply.started":"2022-07-10T08:05:30.350976Z","shell.execute_reply":"2022-07-10T08:05:33.749294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Model\n\n### 4.1. Model creation\n\nWe base our neural network on a VGG (Visual Geometry Group) network. To better analyze images' content, we are gonna use a CNN (Convolutional Neural Network) which will first apply a convolution operator of 3x3 px on the image (one per color channel). Then we apply a max pooling to downsample the input by keeping maximum values and a batch normalization to keep mean around 0 and standard deviation around 1 so the CNN converges faster. After, we apply a dropout of 10% to avoid overfitting by dropping 10% of the nodes. We repeat this cycle 4 times and finally we apply a global average pooling to average each array of data and a dense layer to connect to all neurons. \n\nWe use the relu function for activatio, as it is very performant, on every layer except for the last one where we use a softmax in order to return a probability (*i.e.* a value in $[0,1]$).","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adadelta\n\ninput_shape = (32, 32, 3)\nmodel = Sequential()\n\n#model.add(RandomContrast(0.5, seed=SEED))\n\n# First cycle\nmodel.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu', input_shape=input_shape))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.15))\n\n# Second cycle\nmodel.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.15))\n\n# Third cycle\nmodel.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.15))\n\n# Fourth cycle\nmodel.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(512, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.15))\n\n# Averaging and densing\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dropout(0.15))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(len(label_map), activation='softmax'))\n\nmodel.compile(optimizer=Adadelta(learning_rate=0.05), \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n#model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:33.751947Z","iopub.execute_input":"2022-07-10T08:05:33.75252Z","iopub.status.idle":"2022-07-10T08:05:34.258121Z","shell.execute_reply.started":"2022-07-10T08:05:33.752476Z","shell.execute_reply":"2022-07-10T08:05:34.257187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.2. Model training\n\nWe use the train and validation datasets to train our model using the previously defined image data generators. We will use a callback to keep a trace of the best weights during the training as they will vary and might get inaccurate on the last trainings.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\ncheckpoint_path = '/kaggle/working/model_checkpoint'\nmodel_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,\n                                            save_weights_only=True,\n                                            monitor='val_accuracy',\n                                            mode='max',\n                                            save_best_only=True,\n                                            verbose=1)\n\nepochs = 200\n\nmodel_history = model.fit(x=train_generator,\n                          epochs=epochs,\n                          verbose=1,\n                          initial_epoch=0,\n                          validation_data=validation_generator,\n                          callbacks=[model_checkpoint_callback])","metadata":{"execution":{"iopub.status.busy":"2022-07-10T08:05:34.259963Z","iopub.execute_input":"2022-07-10T08:05:34.260515Z","iopub.status.idle":"2022-07-10T11:50:16.453532Z","shell.execute_reply.started":"2022-07-10T08:05:34.260473Z","shell.execute_reply":"2022-07-10T11:50:16.452608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our model get a quite good accuracy over the validation dataset with 79,747%, but we need to check it against the test dataset to be sure we didn't nake any overfitting.\n\n### 4.3. Model's training results\n\nWe plot bellow a comparaison of the train and validation accuracy and loss over epochs.","metadata":{}},{"cell_type":"code","source":"plt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('Model accuracy over epochs')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.xlim([0, epochs])\nplt.legend(['Train accuracy', 'Validation accuracy'], loc='lower right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:16.455519Z","iopub.execute_input":"2022-07-10T11:50:16.45583Z","iopub.status.idle":"2022-07-10T11:50:16.690798Z","shell.execute_reply.started":"2022-07-10T11:50:16.455788Z","shell.execute_reply":"2022-07-10T11:50:16.689743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.xlim([0, epochs])\nplt.legend(['Train loss', 'Validation loss'], loc='upper right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:16.692522Z","iopub.execute_input":"2022-07-10T11:50:16.692807Z","iopub.status.idle":"2022-07-10T11:50:16.934872Z","shell.execute_reply.started":"2022-07-10T11:50:16.692766Z","shell.execute_reply":"2022-07-10T11:50:16.933823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_train_accuracy = model_history.history['accuracy'][-1]\nmax_val_accuracy = model_history.history['val_accuracy'][-1]\nprint(f\"The maximum accuracy on the train dataset is {round(max_train_accuracy * 100, 2)}%\")\nprint(f\"The maximum accuracy on the validation dataset is {round(max_val_accuracy * 100, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:16.9365Z","iopub.execute_input":"2022-07-10T11:50:16.936826Z","iopub.status.idle":"2022-07-10T11:50:16.943834Z","shell.execute_reply.started":"2022-07-10T11:50:16.936781Z","shell.execute_reply":"2022-07-10T11:50:16.942816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the model converges very quickly on the firsts epochs and then quite slower. The train accuracy reach around 91% on the last epoch, and the validation accuracy around 79%. We can see that the model stuck after the 20th or 30th epoch as its loss starts to freeze. Hopefully we used a callback to prevent overfitting in such situations.\n\n#### 4.3.1. Model accuracy\n\nBelow we compute the accuracy of our model against the test dataset :","metadata":{}},{"cell_type":"code","source":"model.load_weights(checkpoint_path)\nmetric = model.evaluate(test_generator)\nprint(f\"Model's accurary on test dataset : {metric[1] * 100}%\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:16.945732Z","iopub.execute_input":"2022-07-10T11:50:16.946396Z","iopub.status.idle":"2022-07-10T11:50:18.545647Z","shell.execute_reply.started":"2022-07-10T11:50:16.946321Z","shell.execute_reply":"2022-07-10T11:50:18.544593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy against the test dataset is pretty good with 78.38%, considering the small size of images and limited data augmentation techniques.\n\n#### 4.3.2. Confusion matrix\n\nThe model didn't overfit our training and validation dataset which is a very good news.  We will now plot a confusion matrix of our model against the test dataset to see on which categories it has more difficulties. But first we save our model.","metadata":{}},{"cell_type":"code","source":"model.save(filepath='/kaggle/working/model')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:18.548221Z","iopub.execute_input":"2022-07-10T11:50:18.548747Z","iopub.status.idle":"2022-07-10T11:50:24.702339Z","shell.execute_reply.started":"2022-07-10T11:50:18.548704Z","shell.execute_reply":"2022-07-10T11:50:24.701186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we create a small function to plot the confusion matruix using a heatmap to better see the values. The closer to 100 the values on the diagonal, the closest to real label our model is.","metadata":{}},{"cell_type":"code","source":"import seaborn as sn\n\ndef plot_confusion_matrix(conf_matrix, categories, title=\"Confusion matrix\"):\n    confusion_df = pd.DataFrame(conf_matrix, categories, categories)\n    plt.figure(figsize=(10,8))\n    sn.set(font_scale=1.4) # for label size\n    sn.heatmap(confusion_df, cmap='rocket_r', fmt='g', annot=True, annot_kws={\"size\": 12})\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.title(title)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:24.704862Z","iopub.execute_input":"2022-07-10T11:50:24.705201Z","iopub.status.idle":"2022-07-10T11:50:25.09432Z","shell.execute_reply.started":"2022-07-10T11:50:24.705145Z","shell.execute_reply":"2022-07-10T11:50:25.093337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# compute the confusion matrix of the model\ny_pred = model.predict(test_generator).argmax(axis=1)\nconf_matrix = confusion_matrix(test_generator.classes, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:25.095674Z","iopub.execute_input":"2022-07-10T11:50:25.095975Z","iopub.status.idle":"2022-07-10T11:50:26.474177Z","shell.execute_reply.started":"2022-07-10T11:50:25.095931Z","shell.execute_reply":"2022-07-10T11:50:26.4733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(conf_matrix, label_map.values(), title=\"Confusion matrix from the model prediction\")","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:26.475377Z","iopub.execute_input":"2022-07-10T11:50:26.476716Z","iopub.status.idle":"2022-07-10T11:50:27.734169Z","shell.execute_reply.started":"2022-07-10T11:50:26.476672Z","shell.execute_reply":"2022-07-10T11:50:27.732874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see our model struggles with some categories such as corvette vs destroyer or smallfish vs vsmallfish. This can be explain by the poor resolution of the dataset and the fact that those kind of boat are very similar in real life.\n\n## 5. Submission\n\nBellow are the steps to submit to the leaderboard.","metadata":{}},{"cell_type":"code","source":"submission_file = '/kaggle/working/ships_competition.npz'\nX_submission = np.load(submission_file, allow_pickle=True)[\"X\"]\n\n#X_submission = preprocessing(X_submission.astype(float)).numpy()\nX_submission = X_submission.astype(float)/255\n\nres = model.predict(X_submission).argmax(axis=1)\ndf = pd.DataFrame({\"Category\":res})\ndf.to_csv(\"submission.csv\", index_label=\"Id\")\n\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-10T11:50:27.73668Z","iopub.execute_input":"2022-07-10T11:50:27.737084Z","iopub.status.idle":"2022-07-10T11:50:28.351991Z","shell.execute_reply.started":"2022-07-10T11:50:27.737024Z","shell.execute_reply":"2022-07-10T11:50:28.350881Z"},"trusted":true},"execution_count":null,"outputs":[]}]}